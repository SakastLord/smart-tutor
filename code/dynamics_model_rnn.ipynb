{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sketchbook to experiment with RNN models to model the dynamics. Once tested, the code in this notebook will be incorporated into functions/classes/ python modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics Model\n",
    "Models the dynamics of the system p(next observation | history of observations, action)\n",
    "\n",
    "History of observations consist of exercises a student has done and whether the student solved each of them\n",
    "\n",
    "Action is the next exercise chosen \n",
    "\n",
    "Next observation is whether the student gets the chosen exercise correct\n",
    "\n",
    "\n",
    "We want to use an RNN to model the dynamics.\n",
    "Input data represents history of observations, of shape (n_students, n_timesteps, observation_vec_size)\n",
    "\n",
    "Output represents the probability of getting next exercise correctly, of shape (n_students, n_timesteps, n_exercises) \n",
    "\n",
    "So at each timestep, we make a prediction for all actions. \n",
    "\n",
    "For each action, the output vector specifies the predicted probability of the student getting the chosen exercise correctly.\n",
    "\n",
    "The target output only contains binary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lisa1010/tf_venv/bin/python\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print sys.executable\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load toy data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = dataset_utils.load_data(filename=\"../synthetic_data/toy.pickle\")\n",
    "input_data_, output_mask_, target_data_ = dataset_utils.preprocess_data_for_rnn(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu_options {\n",
       "}\n",
       "allow_soft_placement: true"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflearn.init_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 64\n",
    "n_samples, n_timesteps, n_inputdim = input_data_.shape\n",
    "_,_,n_outputdim = target_data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print n_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print n_inputdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print n_outputdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: 2PTSBL\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_2.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "---------------------------------\n",
      "Training samples: 4\n",
      "Validation samples: 1\n",
      "--\n",
      "Training Step: 1  | time: 2.116s\n",
      "| Adam | epoch: 001 | loss: 0.00000 | val_loss: 0.04748 -- iter: 4/4\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.04031\u001b[0m\u001b[0m | time: 1.067s\n",
      "| Adam | epoch: 002 | loss: 0.04031 | val_loss: 0.04669 -- iter: 4/4\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.04343\u001b[0m\u001b[0m | time: 1.065s\n",
      "| Adam | epoch: 003 | loss: 0.04343 | val_loss: 0.04589 -- iter: 4/4\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.04341\u001b[0m\u001b[0m | time: 1.061s\n",
      "| Adam | epoch: 004 | loss: 0.04341 | val_loss: 0.04506 -- iter: 4/4\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.04293\u001b[0m\u001b[0m | time: 1.060s\n",
      "| Adam | epoch: 005 | loss: 0.04293 | val_loss: 0.04419 -- iter: 4/4\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.04231\u001b[0m\u001b[0m | time: 1.056s\n",
      "| Adam | epoch: 006 | loss: 0.04231 | val_loss: 0.04326 -- iter: 4/4\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.04168\u001b[0m\u001b[0m | time: 1.061s\n",
      "| Adam | epoch: 007 | loss: 0.04168 | val_loss: 0.04223 -- iter: 4/4\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.04099\u001b[0m\u001b[0m | time: 1.055s\n",
      "| Adam | epoch: 008 | loss: 0.04099 | val_loss: 0.04111 -- iter: 4/4\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.04024\u001b[0m\u001b[0m | time: 1.059s\n",
      "| Adam | epoch: 009 | loss: 0.04024 | val_loss: 0.03985 -- iter: 4/4\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.03943\u001b[0m\u001b[0m | time: 1.053s\n",
      "| Adam | epoch: 010 | loss: 0.03943 | val_loss: 0.03846 -- iter: 4/4\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "graph_to_use = tf.Graph()\n",
    "with graph_to_use.as_default():\n",
    "    net = tflearn.input_data([None, n_timesteps, n_inputdim],dtype=tf.float32, name='input_data')\n",
    "    output_mask = tflearn.input_data([None, n_timesteps, n_outputdim], dtype=tf.float32, name='output_mask')\n",
    "    net = tflearn.lstm(net, n_hidden, return_seq=True, name=\"lstm_1\")\n",
    "    net = tflearn.lstm(net, n_outputdim, return_seq=True, name=\"lstm_2\")\n",
    "    net = tf.stack(net, axis=1)\n",
    "    preds = net\n",
    "    net = net * output_mask\n",
    "    net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                             loss='mean_square')\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "    model.fit([ input_data_, output_mask_], target_data_, validation_set=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'output_mask:0' shape=(?, 50, 10) dtype=int32>,\n",
       " <tf.Tensor 'InputData/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_1:0' shape=(?, 50, 10) dtype=int32>,\n",
       " <tf.Tensor 'InputData_1/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_2:0' shape=(?, 50, 10) dtype=int32>,\n",
       " <tf.Tensor 'InputData_2/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_3:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'InputData_3/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'input_data:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_4:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'input_data_1:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_5:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'input_data_2:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_6:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'input_data_3:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_7:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'input_data_4:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_8:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_9:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'input_data_5/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'input_data_6/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_10/X:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_9:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'input_data_7/X:0' shape=(?, 50, 20) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_11/X:0' shape=(?, 50, 10) dtype=float32>,\n",
       " <tf.Tensor 'output_mask_9:0' shape=(?, 50, 10) dtype=float32>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.INPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = dataset_utils.load_data(filename=\"../synthetic_data/1000stud_100seq_expert.pickle\")\n",
    "input_data_, output_mask_, target_data_ = dataset_utils.preprocess_data_for_rnn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100\n",
      "20\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_timesteps, n_inputdim = input_data_.shape\n",
    "_,_,n_outputdim = target_data_.shape\n",
    "print n_samples\n",
    "print n_timesteps\n",
    "print n_inputdim\n",
    "print n_outputdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.02480\u001b[0m\u001b[0m | time: 2.358s\n",
      "| Adam | epoch: 032 | loss: 0.02480 -- iter: 896/900\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.02479\u001b[0m\u001b[0m | time: 3.469s\n",
      "| Adam | epoch: 032 | loss: 0.02479 | val_loss: 0.02476 -- iter: 900/900\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "graph_to_use = tf.Graph()\n",
    "with graph_to_use.as_default():\n",
    "    net = tflearn.input_data([None, n_timesteps, n_inputdim],dtype=tf.float32, name='input_data')\n",
    "    output_mask = tflearn.input_data([None, n_timesteps, n_outputdim], dtype=tf.float32, name='output_mask')\n",
    "    net = tflearn.lstm(net, n_hidden, return_seq=True, name=\"lstm_1\")\n",
    "    net = tflearn.lstm(net, n_outputdim, return_seq=True, name=\"lstm_2\")\n",
    "    net = tf.stack(net, axis=1)\n",
    "    preds = net\n",
    "    net = net * output_mask\n",
    "    net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                             loss='mean_square')\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=2)\n",
    "    model.fit([ input_data_, output_mask_], target_data_, n_epoch=32, validation_set=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dynamics_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RNN dynamics model...\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test_model/\n",
      "Checkpoint directory path: ../checkpoints/test_model/\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_id=\"test_model\", load_checkpoint=False, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = (input_data_[:,:10,:], output_mask_[:,:10,:], target_data_[:,:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.03538\u001b[0m\u001b[0m | time: 0.681s\n",
      "| Adam | epoch: 003 | loss: 0.03538 -- iter: 896/900\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.03482\u001b[0m\u001b[0m | time: 1.726s\n",
      "| Adam | epoch: 003 | loss: 0.03482 | val_loss: 0.03224 -- iter: 900/900\n",
      "--\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-175a1436c9bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/lisa1010/dev/smart-tutor/code/dynamics_model.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, load_checkpoint)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mdate_time_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%m-%d-%Y_%H-%M-%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mrun_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_time_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/models/dnn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    213\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                     \u001b[0;31m# Epoch end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                     \u001b[0mcaller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, training_state)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, training_state)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/callbacks.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, training_step)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnapshot_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnapshot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, model_file, global_step)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_saver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1373\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[1;32m   1374\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices)\u001b[0m\n\u001b[1;32m   1401\u001b[0m     return export_meta_graph(\n\u001b[1;32m   1402\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0mgraph_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m         \u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0mcollection_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mas_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   2187\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mtoo\u001b[0m \u001b[0mlarge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m     \"\"\"\n\u001b[0;32m-> 2189\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_as_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   2146\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m\"_output_shapes\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m             graph.node[-1].attr[\"_output_shapes\"].list.shape.extend([\n\u001b[0m\u001b[1;32m   2149\u001b[0m                 output.get_shape().as_proto() for output in op.outputs])\n\u001b[1;32m   2150\u001b[0m           \u001b[0mbytesize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mgetter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0;31m# Construct a new object to represent this field.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0mfield_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       \u001b[0;31m# Atomically check if another thread has preempted us and, if not, swap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mMakeSubMessageDefault\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    424\u001b[0m       result._SetListener(\n\u001b[1;32m    425\u001b[0m           \u001b[0m_OneofListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontaining_oneof\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m           else message._listener_for_children)\n\u001b[1;32m    428\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lisa1010/tf_venv/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent_message, field)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       \u001b[0mfield\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfield\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparent\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \"\"\"\n\u001b[0;32m-> 1409\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_OneofListener\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[-0.007385450880974531,\n",
       "   0.025550365447998047,\n",
       "   0.011329921893775463,\n",
       "   0.03704448416829109,\n",
       "   0.039401981979608536,\n",
       "   0.027618983760476112,\n",
       "   0.017678581178188324,\n",
       "   0.025421805679798126,\n",
       "   0.01569029688835144,\n",
       "   -0.02289711870253086],\n",
       "  [-0.014513563364744186,\n",
       "   0.0470220223069191,\n",
       "   0.040145840495824814,\n",
       "   0.09161123633384705,\n",
       "   0.10280773788690567,\n",
       "   0.06935103237628937,\n",
       "   0.05226275697350502,\n",
       "   0.041719403117895126,\n",
       "   0.04879506677389145,\n",
       "   -0.07392476499080658],\n",
       "  [-0.03218340128660202,\n",
       "   0.07135201245546341,\n",
       "   0.0685245469212532,\n",
       "   0.1436130404472351,\n",
       "   0.19665291905403137,\n",
       "   0.1438368409872055,\n",
       "   0.1049000471830368,\n",
       "   0.06255870312452316,\n",
       "   0.1026017889380455,\n",
       "   -0.11450488120317459],\n",
       "  [-0.06921423971652985,\n",
       "   0.08478009700775146,\n",
       "   0.1043052151799202,\n",
       "   0.19068250060081482,\n",
       "   0.2767428159713745,\n",
       "   0.23720687627792358,\n",
       "   0.17555317282676697,\n",
       "   0.10750668495893478,\n",
       "   0.16081872582435608,\n",
       "   -0.14396199584007263],\n",
       "  [-0.11939936876296997,\n",
       "   0.10138699412345886,\n",
       "   0.13752755522727966,\n",
       "   0.24032481014728546,\n",
       "   0.3501714766025543,\n",
       "   0.32600459456443787,\n",
       "   0.26092734932899475,\n",
       "   0.16472099721431732,\n",
       "   0.22535595297813416,\n",
       "   -0.17225711047649384],\n",
       "  [-0.17461079359054565,\n",
       "   0.1152421087026596,\n",
       "   0.1707264631986618,\n",
       "   0.2937884032726288,\n",
       "   0.4026610255241394,\n",
       "   0.4047226011753082,\n",
       "   0.3470379114151001,\n",
       "   0.2304689586162567,\n",
       "   0.2942657470703125,\n",
       "   -0.20128174126148224],\n",
       "  [-0.23769868910312653,\n",
       "   0.1092974990606308,\n",
       "   0.2082236111164093,\n",
       "   0.3569919466972351,\n",
       "   0.4412563443183899,\n",
       "   0.4894976019859314,\n",
       "   0.422652930021286,\n",
       "   0.32053667306900024,\n",
       "   0.35850971937179565,\n",
       "   -0.20820315182209015],\n",
       "  [-0.2928321361541748,\n",
       "   0.11726514250040054,\n",
       "   0.22756768763065338,\n",
       "   0.41967979073524475,\n",
       "   0.4618801772594452,\n",
       "   0.5404962301254272,\n",
       "   0.47515639662742615,\n",
       "   0.4064711928367615,\n",
       "   0.41023027896881104,\n",
       "   -0.218476802110672],\n",
       "  [-0.34365835785865784,\n",
       "   0.11928804963827133,\n",
       "   0.2321077436208725,\n",
       "   0.4715743064880371,\n",
       "   0.4840993285179138,\n",
       "   0.5689555406570435,\n",
       "   0.5109775066375732,\n",
       "   0.4563242495059967,\n",
       "   0.46420493721961975,\n",
       "   -0.2140037715435028],\n",
       "  [-0.38566485047340393,\n",
       "   0.1281179040670395,\n",
       "   0.24280832707881927,\n",
       "   0.5128179788589478,\n",
       "   0.48435699939727783,\n",
       "   0.6077094674110413,\n",
       "   0.544141948223114,\n",
       "   0.49809062480926514,\n",
       "   0.5067498683929443,\n",
       "   -0.2280215620994568]],\n",
       " [[0.001626665354706347,\n",
       "   0.037479467689991,\n",
       "   0.019430629909038544,\n",
       "   0.03404674306511879,\n",
       "   0.012370062991976738,\n",
       "   0.03443584218621254,\n",
       "   0.023976050317287445,\n",
       "   0.009455711580812931,\n",
       "   0.026551280170679092,\n",
       "   -0.015678660944104195],\n",
       "  [-0.014866768382489681,\n",
       "   0.07031739503145218,\n",
       "   0.037209831178188324,\n",
       "   0.08887828141450882,\n",
       "   0.06364879757165909,\n",
       "   0.08181287348270416,\n",
       "   0.05092545226216316,\n",
       "   0.03311295807361603,\n",
       "   0.051180221140384674,\n",
       "   -0.04780920222401619],\n",
       "  [-0.03127816319465637,\n",
       "   0.09107029438018799,\n",
       "   0.07287603616714478,\n",
       "   0.15671797096729279,\n",
       "   0.1430317461490631,\n",
       "   0.14184020459651947,\n",
       "   0.09511943906545639,\n",
       "   0.050354160368442535,\n",
       "   0.09149425476789474,\n",
       "   -0.11039866507053375],\n",
       "  [-0.060976751148700714,\n",
       "   0.09801477193832397,\n",
       "   0.11476847529411316,\n",
       "   0.24379442632198334,\n",
       "   0.2399071305990219,\n",
       "   0.21377906203269958,\n",
       "   0.16034163534641266,\n",
       "   0.07652704417705536,\n",
       "   0.14679443836212158,\n",
       "   -0.17103984951972961],\n",
       "  [-0.10324911773204803,\n",
       "   0.11362592875957489,\n",
       "   0.1525913029909134,\n",
       "   0.31725165247917175,\n",
       "   0.34183502197265625,\n",
       "   0.3059535622596741,\n",
       "   0.24026139080524445,\n",
       "   0.1149774044752121,\n",
       "   0.22379063069820404,\n",
       "   -0.2214277982711792],\n",
       "  [-0.1607619673013687,\n",
       "   0.12293191254138947,\n",
       "   0.1840815544128418,\n",
       "   0.38961559534072876,\n",
       "   0.40708380937576294,\n",
       "   0.4095291197299957,\n",
       "   0.3294735550880432,\n",
       "   0.18316788971424103,\n",
       "   0.2974473237991333,\n",
       "   -0.26686352491378784],\n",
       "  [-0.2251933366060257,\n",
       "   0.129822239279747,\n",
       "   0.21352751553058624,\n",
       "   0.45145541429519653,\n",
       "   0.44791433215141296,\n",
       "   0.5020326375961304,\n",
       "   0.4140762388706207,\n",
       "   0.2700747847557068,\n",
       "   0.3686193525791168,\n",
       "   -0.3029167652130127],\n",
       "  [-0.2910039722919464,\n",
       "   0.1354573369026184,\n",
       "   0.24534107744693756,\n",
       "   0.4955673813819885,\n",
       "   0.47051355242729187,\n",
       "   0.5658157467842102,\n",
       "   0.4853389859199524,\n",
       "   0.3722583055496216,\n",
       "   0.43880945444107056,\n",
       "   -0.3212301731109619],\n",
       "  [-0.3583813011646271,\n",
       "   0.1330350786447525,\n",
       "   0.279346227645874,\n",
       "   0.5353736877441406,\n",
       "   0.49029308557510376,\n",
       "   0.6222624182701111,\n",
       "   0.5359885692596436,\n",
       "   0.47413772344589233,\n",
       "   0.4909423291683197,\n",
       "   -0.30825361609458923],\n",
       "  [-0.4008702039718628,\n",
       "   0.14655952155590057,\n",
       "   0.29526978731155396,\n",
       "   0.5669235587120056,\n",
       "   0.4961823523044586,\n",
       "   0.6476258635520935,\n",
       "   0.5633453726768494,\n",
       "   0.547645092010498,\n",
       "   0.5222209692001343,\n",
       "   -0.30059829354286194]],\n",
       " [[0.001626665354706347,\n",
       "   0.037479467689991,\n",
       "   0.019430629909038544,\n",
       "   0.03404674306511879,\n",
       "   0.012370062991976738,\n",
       "   0.03443584218621254,\n",
       "   0.023976050317287445,\n",
       "   0.009455711580812931,\n",
       "   0.026551280170679092,\n",
       "   -0.015678660944104195],\n",
       "  [-0.005862789694219828,\n",
       "   0.08617755770683289,\n",
       "   0.04532817006111145,\n",
       "   0.0842113122344017,\n",
       "   0.03539608418941498,\n",
       "   0.08861597627401352,\n",
       "   0.05721341073513031,\n",
       "   0.0165015310049057,\n",
       "   0.06334442645311356,\n",
       "   -0.03913375735282898],\n",
       "  [-0.031976114958524704,\n",
       "   0.1187528744339943,\n",
       "   0.07019568234682083,\n",
       "   0.15418589115142822,\n",
       "   0.10436864197254181,\n",
       "   0.15436528623104095,\n",
       "   0.09397280961275101,\n",
       "   0.04188372194766998,\n",
       "   0.0956481471657753,\n",
       "   -0.08248429745435715],\n",
       "  [-0.058634355664253235,\n",
       "   0.1337626725435257,\n",
       "   0.11179366707801819,\n",
       "   0.23167745769023895,\n",
       "   0.1990850567817688,\n",
       "   0.22867818176746368,\n",
       "   0.14849594235420227,\n",
       "   0.06368935108184814,\n",
       "   0.1434764862060547,\n",
       "   -0.15573088824748993],\n",
       "  [-0.09788542985916138,\n",
       "   0.14540064334869385,\n",
       "   0.1553751826286316,\n",
       "   0.30297383666038513,\n",
       "   0.3119584918022156,\n",
       "   0.3226514458656311,\n",
       "   0.22293721139431,\n",
       "   0.0977112278342247,\n",
       "   0.21700194478034973,\n",
       "   -0.21283042430877686],\n",
       "  [-0.1556023359298706,\n",
       "   0.14597055315971375,\n",
       "   0.19833093881607056,\n",
       "   0.36874639987945557,\n",
       "   0.38667625188827515,\n",
       "   0.4158157408237457,\n",
       "   0.3109106123447418,\n",
       "   0.16846641898155212,\n",
       "   0.2929220497608185,\n",
       "   -0.2547641396522522],\n",
       "  [-0.22068890929222107,\n",
       "   0.15480518341064453,\n",
       "   0.23436932265758514,\n",
       "   0.4227451980113983,\n",
       "   0.4372327923774719,\n",
       "   0.4868674576282501,\n",
       "   0.3958335220813751,\n",
       "   0.2547925114631653,\n",
       "   0.36704960465431213,\n",
       "   -0.29151248931884766],\n",
       "  [-0.29112106561660767,\n",
       "   0.1480877697467804,\n",
       "   0.27615466713905334,\n",
       "   0.4806373417377472,\n",
       "   0.46889474987983704,\n",
       "   0.5629793405532837,\n",
       "   0.47029396891593933,\n",
       "   0.36612704396247864,\n",
       "   0.436046302318573,\n",
       "   -0.2990073561668396],\n",
       "  [-0.3489680290222168,\n",
       "   0.15215113759040833,\n",
       "   0.29507625102996826,\n",
       "   0.5293710827827454,\n",
       "   0.47888946533203125,\n",
       "   0.6004736423492432,\n",
       "   0.5143836140632629,\n",
       "   0.4624844491481781,\n",
       "   0.4735715985298157,\n",
       "   -0.3080124258995056],\n",
       "  [-0.4017753005027771,\n",
       "   0.16196505725383759,\n",
       "   0.3105895519256592,\n",
       "   0.564799427986145,\n",
       "   0.4860957860946655,\n",
       "   0.6261220574378967,\n",
       "   0.5451611280441284,\n",
       "   0.5394534468650818,\n",
       "   0.5054194927215576,\n",
       "   -0.312355101108551]],\n",
       " [[0.001626665354706347,\n",
       "   0.037479467689991,\n",
       "   0.019430629909038544,\n",
       "   0.03404674306511879,\n",
       "   0.012370062991976738,\n",
       "   0.03443584218621254,\n",
       "   0.023976050317287445,\n",
       "   0.009455711580812931,\n",
       "   0.026551280170679092,\n",
       "   -0.015678660944104195],\n",
       "  [-0.014866768382489681,\n",
       "   0.07031739503145218,\n",
       "   0.037209831178188324,\n",
       "   0.08887828141450882,\n",
       "   0.06364879757165909,\n",
       "   0.08181287348270416,\n",
       "   0.05092545226216316,\n",
       "   0.03311295807361603,\n",
       "   0.051180221140384674,\n",
       "   -0.04780920222401619],\n",
       "  [-0.03420348837971687,\n",
       "   0.08495858311653137,\n",
       "   0.0664568692445755,\n",
       "   0.15616393089294434,\n",
       "   0.13940481841564178,\n",
       "   0.1283305585384369,\n",
       "   0.08701346069574356,\n",
       "   0.05158423259854317,\n",
       "   0.08342994749546051,\n",
       "   -0.08944535255432129],\n",
       "  [-0.060907602310180664,\n",
       "   0.09095901250839233,\n",
       "   0.10269273072481155,\n",
       "   0.232721745967865,\n",
       "   0.23196962475776672,\n",
       "   0.18219909071922302,\n",
       "   0.13328084349632263,\n",
       "   0.0728285014629364,\n",
       "   0.12778891623020172,\n",
       "   -0.14081640541553497],\n",
       "  [-0.09361746907234192,\n",
       "   0.0981183871626854,\n",
       "   0.14935459196567535,\n",
       "   0.3159990906715393,\n",
       "   0.3259636163711548,\n",
       "   0.25963741540908813,\n",
       "   0.20035579800605774,\n",
       "   0.10048488527536392,\n",
       "   0.1935679167509079,\n",
       "   -0.22298680245876312],\n",
       "  [-0.1449628323316574,\n",
       "   0.09888078272342682,\n",
       "   0.19733908772468567,\n",
       "   0.40551644563674927,\n",
       "   0.4001621603965759,\n",
       "   0.3491721451282501,\n",
       "   0.2830325663089752,\n",
       "   0.14919288456439972,\n",
       "   0.2731110155582428,\n",
       "   -0.2958981990814209],\n",
       "  [-0.20487850904464722,\n",
       "   0.10200666636228561,\n",
       "   0.24171984195709229,\n",
       "   0.4854142367839813,\n",
       "   0.44695594906806946,\n",
       "   0.4349038898944855,\n",
       "   0.36876046657562256,\n",
       "   0.21863026916980743,\n",
       "   0.3542357385158539,\n",
       "   -0.36073917150497437],\n",
       "  [-0.2662322223186493,\n",
       "   0.12092211842536926,\n",
       "   0.28500714898109436,\n",
       "   0.5353193879127502,\n",
       "   0.4847032427787781,\n",
       "   0.5113058090209961,\n",
       "   0.44373786449432373,\n",
       "   0.3023030459880829,\n",
       "   0.43968331813812256,\n",
       "   -0.4079033434391022],\n",
       "  [-0.3269027769565582,\n",
       "   0.13777515292167664,\n",
       "   0.3185170888900757,\n",
       "   0.5725964307785034,\n",
       "   0.49580419063568115,\n",
       "   0.5755017995834351,\n",
       "   0.5088567733764648,\n",
       "   0.40885812044143677,\n",
       "   0.5052441358566284,\n",
       "   -0.4334279000759125],\n",
       "  [-0.3791249692440033,\n",
       "   0.16812054812908173,\n",
       "   0.3467302620410919,\n",
       "   0.5862396955490112,\n",
       "   0.5050480365753174,\n",
       "   0.6146808862686157,\n",
       "   0.546154797077179,\n",
       "   0.49721837043762207,\n",
       "   0.5499317646026611,\n",
       "   -0.4526500999927521]],\n",
       " [[-0.007385450880974531,\n",
       "   0.025550365447998047,\n",
       "   0.011329921893775463,\n",
       "   0.03704448416829109,\n",
       "   0.039401981979608536,\n",
       "   0.027618983760476112,\n",
       "   0.017678581178188324,\n",
       "   0.025421805679798126,\n",
       "   0.01569029688835144,\n",
       "   -0.02289711870253086],\n",
       "  [-0.016434617340564728,\n",
       "   0.04202163591980934,\n",
       "   0.03387784957885742,\n",
       "   0.09133366495370865,\n",
       "   0.09891992807388306,\n",
       "   0.05646282434463501,\n",
       "   0.04472542181611061,\n",
       "   0.0426873117685318,\n",
       "   0.04018104821443558,\n",
       "   -0.05347241833806038],\n",
       "  [-0.02927660197019577,\n",
       "   0.058802951127290726,\n",
       "   0.07028387486934662,\n",
       "   0.16001532971858978,\n",
       "   0.18033723533153534,\n",
       "   0.10849107801914215,\n",
       "   0.09034308046102524,\n",
       "   0.057852793484926224,\n",
       "   0.0845632553100586,\n",
       "   -0.11631344258785248],\n",
       "  [-0.057247862219810486,\n",
       "   0.08008474111557007,\n",
       "   0.10546114295721054,\n",
       "   0.2235705405473709,\n",
       "   0.2839057445526123,\n",
       "   0.19722367823123932,\n",
       "   0.15593916177749634,\n",
       "   0.0836041271686554,\n",
       "   0.15329524874687195,\n",
       "   -0.1659594476222992],\n",
       "  [-0.10434599965810776,\n",
       "   0.09520581364631653,\n",
       "   0.1370006948709488,\n",
       "   0.29061993956565857,\n",
       "   0.3641490042209625,\n",
       "   0.3098030984401703,\n",
       "   0.24149750173091888,\n",
       "   0.13650289177894592,\n",
       "   0.22524765133857727,\n",
       "   -0.21232809126377106],\n",
       "  [-0.167626291513443,\n",
       "   0.10245420038700104,\n",
       "   0.17349250614643097,\n",
       "   0.34856370091438293,\n",
       "   0.41738319396972656,\n",
       "   0.4118785858154297,\n",
       "   0.3323492109775543,\n",
       "   0.21695390343666077,\n",
       "   0.30281487107276917,\n",
       "   -0.2412136197090149],\n",
       "  [-0.24291224777698517,\n",
       "   0.10032673925161362,\n",
       "   0.208566814661026,\n",
       "   0.40867307782173157,\n",
       "   0.4561443626880646,\n",
       "   0.5027825236320496,\n",
       "   0.41346457600593567,\n",
       "   0.32292020320892334,\n",
       "   0.37091270089149475,\n",
       "   -0.24030572175979614],\n",
       "  [-0.3041929602622986,\n",
       "   0.11394774913787842,\n",
       "   0.2258629947900772,\n",
       "   0.46385255455970764,\n",
       "   0.47372329235076904,\n",
       "   0.5544580817222595,\n",
       "   0.4730626344680786,\n",
       "   0.4212020933628082,\n",
       "   0.42455315589904785,\n",
       "   -0.24243329465389252],\n",
       "  [-0.3572836220264435,\n",
       "   0.1321299821138382,\n",
       "   0.22877410054206848,\n",
       "   0.5150938034057617,\n",
       "   0.489890992641449,\n",
       "   0.5821978449821472,\n",
       "   0.508150041103363,\n",
       "   0.47752436995506287,\n",
       "   0.4684116542339325,\n",
       "   -0.25712013244628906],\n",
       "  [-0.40285348892211914,\n",
       "   0.1360579878091812,\n",
       "   0.2240927666425705,\n",
       "   0.5503410696983337,\n",
       "   0.507268488407135,\n",
       "   0.602586567401886,\n",
       "   0.5363503098487854,\n",
       "   0.5220065116882324,\n",
       "   0.5163953304290771,\n",
       "   -0.25548022985458374]],\n",
       " [[0.001626665354706347,\n",
       "   0.037479467689991,\n",
       "   0.019430629909038544,\n",
       "   0.03404674306511879,\n",
       "   0.012370062991976738,\n",
       "   0.03443584218621254,\n",
       "   0.023976050317287445,\n",
       "   0.009455711580812931,\n",
       "   0.026551280170679092,\n",
       "   -0.015678660944104195],\n",
       "  [-0.005862789694219828,\n",
       "   0.08617755770683289,\n",
       "   0.04532817006111145,\n",
       "   0.0842113122344017,\n",
       "   0.03539608418941498,\n",
       "   0.08861597627401352,\n",
       "   0.05721341073513031,\n",
       "   0.0165015310049057,\n",
       "   0.06334442645311356,\n",
       "   -0.03913375735282898],\n",
       "  [-0.023123623803257942,\n",
       "   0.13784949481487274,\n",
       "   0.07789395749568939,\n",
       "   0.1472705751657486,\n",
       "   0.07509306073188782,\n",
       "   0.1604142040014267,\n",
       "   0.09974164515733719,\n",
       "   0.02444310672581196,\n",
       "   0.10856691002845764,\n",
       "   -0.07207928597927094],\n",
       "  [-0.05961983650922775,\n",
       "   0.16516649723052979,\n",
       "   0.10900723189115524,\n",
       "   0.22902508080005646,\n",
       "   0.1622152030467987,\n",
       "   0.2404518872499466,\n",
       "   0.14683611690998077,\n",
       "   0.05489376187324524,\n",
       "   0.1488243192434311,\n",
       "   -0.12612617015838623],\n",
       "  [-0.09663838893175125,\n",
       "   0.17179463803768158,\n",
       "   0.15472890436649323,\n",
       "   0.31104403734207153,\n",
       "   0.2644428312778473,\n",
       "   0.32147520780563354,\n",
       "   0.21147173643112183,\n",
       "   0.08512993156909943,\n",
       "   0.20477068424224854,\n",
       "   -0.20753706991672516],\n",
       "  [-0.1476699709892273,\n",
       "   0.16157129406929016,\n",
       "   0.2033478319644928,\n",
       "   0.4017667770385742,\n",
       "   0.3551658093929291,\n",
       "   0.40257886052131653,\n",
       "   0.2912360429763794,\n",
       "   0.13407102227210999,\n",
       "   0.2749353349208832,\n",
       "   -0.2794758379459381],\n",
       "  [-0.20422206819057465,\n",
       "   0.16578540205955505,\n",
       "   0.25083914399147034,\n",
       "   0.46711477637290955,\n",
       "   0.43075132369995117,\n",
       "   0.47971758246421814,\n",
       "   0.3764747381210327,\n",
       "   0.2016037255525589,\n",
       "   0.36287787556648254,\n",
       "   -0.33587899804115295],\n",
       "  [-0.26813051104545593,\n",
       "   0.1662045270204544,\n",
       "   0.28966930508613586,\n",
       "   0.5192147493362427,\n",
       "   0.46377167105674744,\n",
       "   0.5471265316009521,\n",
       "   0.45559510588645935,\n",
       "   0.3056398928165436,\n",
       "   0.44026315212249756,\n",
       "   -0.37334367632865906],\n",
       "  [-0.3387893736362457,\n",
       "   0.16086536645889282,\n",
       "   0.3295435309410095,\n",
       "   0.5598940849304199,\n",
       "   0.4894082844257355,\n",
       "   0.6116238236427307,\n",
       "   0.5161793231964111,\n",
       "   0.4252799153327942,\n",
       "   0.5001445412635803,\n",
       "   -0.37833917140960693],\n",
       "  [-0.38768890500068665,\n",
       "   0.1690857857465744,\n",
       "   0.351093590259552,\n",
       "   0.5892294645309448,\n",
       "   0.49744564294815063,\n",
       "   0.6454594731330872,\n",
       "   0.5497063398361206,\n",
       "   0.5187475085258484,\n",
       "   0.5375459790229797,\n",
       "   -0.3840266764163971]],\n",
       " [[-0.007385450880974531,\n",
       "   0.025550365447998047,\n",
       "   0.011329921893775463,\n",
       "   0.03704448416829109,\n",
       "   0.039401981979608536,\n",
       "   0.027618983760476112,\n",
       "   0.017678581178188324,\n",
       "   0.025421805679798126,\n",
       "   0.01569029688835144,\n",
       "   -0.02289711870253086],\n",
       "  [-0.016434617340564728,\n",
       "   0.04202163591980934,\n",
       "   0.03387784957885742,\n",
       "   0.09133366495370865,\n",
       "   0.09891992807388306,\n",
       "   0.05646282434463501,\n",
       "   0.04472542181611061,\n",
       "   0.0426873117685318,\n",
       "   0.04018104821443558,\n",
       "   -0.05347241833806038],\n",
       "  [-0.02927660197019577,\n",
       "   0.058802951127290726,\n",
       "   0.07028387486934662,\n",
       "   0.16001532971858978,\n",
       "   0.18033723533153534,\n",
       "   0.10849107801914215,\n",
       "   0.09034308046102524,\n",
       "   0.057852793484926224,\n",
       "   0.0845632553100586,\n",
       "   -0.11631344258785248],\n",
       "  [-0.058248504996299744,\n",
       "   0.0690665915608406,\n",
       "   0.11151129007339478,\n",
       "   0.2467028647661209,\n",
       "   0.2727585434913635,\n",
       "   0.17967411875724792,\n",
       "   0.15757513046264648,\n",
       "   0.08372584730386734,\n",
       "   0.1453004628419876,\n",
       "   -0.1782950609922409],\n",
       "  [-0.10167629271745682,\n",
       "   0.08879223465919495,\n",
       "   0.14719465374946594,\n",
       "   0.31988584995269775,\n",
       "   0.3662298917770386,\n",
       "   0.2775735855102539,\n",
       "   0.23900474607944489,\n",
       "   0.12316052615642548,\n",
       "   0.22674421966075897,\n",
       "   -0.22890068590641022],\n",
       "  [-0.1624550074338913,\n",
       "   0.09984000772237778,\n",
       "   0.18285717070102692,\n",
       "   0.3849408030509949,\n",
       "   0.42069122195243835,\n",
       "   0.37895315885543823,\n",
       "   0.3279584050178528,\n",
       "   0.19788342714309692,\n",
       "   0.3057538866996765,\n",
       "   -0.26531723141670227],\n",
       "  [-0.23845139145851135,\n",
       "   0.10098498314619064,\n",
       "   0.2175826132297516,\n",
       "   0.4453183114528656,\n",
       "   0.4605388939380646,\n",
       "   0.47577205300331116,\n",
       "   0.40886789560317993,\n",
       "   0.3037096858024597,\n",
       "   0.3751377761363983,\n",
       "   -0.27267539501190186],\n",
       "  [-0.30577918887138367,\n",
       "   0.11194641143083572,\n",
       "   0.23216237127780914,\n",
       "   0.4968763589859009,\n",
       "   0.474517285823822,\n",
       "   0.530599057674408,\n",
       "   0.4657686650753021,\n",
       "   0.4047018885612488,\n",
       "   0.42059555649757385,\n",
       "   -0.27866390347480774],\n",
       "  [-0.3670070171356201,\n",
       "   0.12681278586387634,\n",
       "   0.24325476586818695,\n",
       "   0.5356999039649963,\n",
       "   0.48277804255485535,\n",
       "   0.5690165162086487,\n",
       "   0.507861852645874,\n",
       "   0.4898432195186615,\n",
       "   0.45843878388404846,\n",
       "   -0.2800208032131195],\n",
       "  [-0.4167822003364563,\n",
       "   0.14109325408935547,\n",
       "   0.250868558883667,\n",
       "   0.5658891201019287,\n",
       "   0.4877112805843353,\n",
       "   0.5969432592391968,\n",
       "   0.5366802215576172,\n",
       "   0.5557106733322144,\n",
       "   0.4856134355068207,\n",
       "   -0.2771853506565094]],\n",
       " [[-0.007385450880974531,\n",
       "   0.025550365447998047,\n",
       "   0.011329921893775463,\n",
       "   0.03704448416829109,\n",
       "   0.039401981979608536,\n",
       "   0.027618983760476112,\n",
       "   0.017678581178188324,\n",
       "   0.025421805679798126,\n",
       "   0.01569029688835144,\n",
       "   -0.02289711870253086],\n",
       "  [-0.016434617340564728,\n",
       "   0.04202163591980934,\n",
       "   0.03387784957885742,\n",
       "   0.09133366495370865,\n",
       "   0.09891992807388306,\n",
       "   0.05646282434463501,\n",
       "   0.04472542181611061,\n",
       "   0.0426873117685318,\n",
       "   0.04018104821443558,\n",
       "   -0.05347241833806038],\n",
       "  [-0.02927660197019577,\n",
       "   0.058802951127290726,\n",
       "   0.07028387486934662,\n",
       "   0.16001532971858978,\n",
       "   0.18033723533153534,\n",
       "   0.10849107801914215,\n",
       "   0.09034308046102524,\n",
       "   0.057852793484926224,\n",
       "   0.0845632553100586,\n",
       "   -0.11631344258785248],\n",
       "  [-0.058248504996299744,\n",
       "   0.0690665915608406,\n",
       "   0.11151129007339478,\n",
       "   0.2467028647661209,\n",
       "   0.2727585434913635,\n",
       "   0.17967411875724792,\n",
       "   0.15757513046264648,\n",
       "   0.08372584730386734,\n",
       "   0.1453004628419876,\n",
       "   -0.1782950609922409],\n",
       "  [-0.10211649537086487,\n",
       "   0.07676362991333008,\n",
       "   0.15105488896369934,\n",
       "   0.34038570523262024,\n",
       "   0.3536534905433655,\n",
       "   0.26201990246772766,\n",
       "   0.23862825334072113,\n",
       "   0.12332990765571594,\n",
       "   0.21722912788391113,\n",
       "   -0.23994718492031097],\n",
       "  [-0.1574753373861313,\n",
       "   0.0961245521903038,\n",
       "   0.18575678765773773,\n",
       "   0.412643164396286,\n",
       "   0.42478057742118835,\n",
       "   0.35893985629081726,\n",
       "   0.32350555062294006,\n",
       "   0.17743146419525146,\n",
       "   0.3039356470108032,\n",
       "   -0.2907536029815674],\n",
       "  [-0.22027722001075745,\n",
       "   0.11041035503149033,\n",
       "   0.2142597734928131,\n",
       "   0.4794926047325134,\n",
       "   0.45950832962989807,\n",
       "   0.46049273014068604,\n",
       "   0.40720677375793457,\n",
       "   0.25972336530685425,\n",
       "   0.3760809004306793,\n",
       "   -0.3337879776954651],\n",
       "  [-0.2838820815086365,\n",
       "   0.12443418055772781,\n",
       "   0.24316547811031342,\n",
       "   0.528627872467041,\n",
       "   0.48025697469711304,\n",
       "   0.5452204942703247,\n",
       "   0.47800135612487793,\n",
       "   0.35232090950012207,\n",
       "   0.43860000371932983,\n",
       "   -0.3651861548423767],\n",
       "  [-0.34057822823524475,\n",
       "   0.13886594772338867,\n",
       "   0.27253884077072144,\n",
       "   0.5640837550163269,\n",
       "   0.4918398857116699,\n",
       "   0.607871949672699,\n",
       "   0.5323774814605713,\n",
       "   0.4408590495586395,\n",
       "   0.48914486169815063,\n",
       "   -0.3854181170463562],\n",
       "  [-0.38752374053001404,\n",
       "   0.15441176295280457,\n",
       "   0.3025898039340973,\n",
       "   0.5882860422134399,\n",
       "   0.49890464544296265,\n",
       "   0.6517040729522705,\n",
       "   0.5718855857849121,\n",
       "   0.5152390003204346,\n",
       "   0.5280877351760864,\n",
       "   -0.3967254161834717]],\n",
       " [[0.001626665354706347,\n",
       "   0.037479467689991,\n",
       "   0.019430629909038544,\n",
       "   0.03404674306511879,\n",
       "   0.012370062991976738,\n",
       "   0.03443584218621254,\n",
       "   0.023976050317287445,\n",
       "   0.009455711580812931,\n",
       "   0.026551280170679092,\n",
       "   -0.015678660944104195],\n",
       "  [-0.014866768382489681,\n",
       "   0.07031739503145218,\n",
       "   0.037209831178188324,\n",
       "   0.08887828141450882,\n",
       "   0.06364879757165909,\n",
       "   0.08181287348270416,\n",
       "   0.05092545226216316,\n",
       "   0.03311295807361603,\n",
       "   0.051180221140384674,\n",
       "   -0.04780920222401619],\n",
       "  [-0.03127816319465637,\n",
       "   0.09107029438018799,\n",
       "   0.07287603616714478,\n",
       "   0.15671797096729279,\n",
       "   0.1430317461490631,\n",
       "   0.14184020459651947,\n",
       "   0.09511943906545639,\n",
       "   0.050354160368442535,\n",
       "   0.09149425476789474,\n",
       "   -0.11039866507053375],\n",
       "  [-0.059480901807546616,\n",
       "   0.11047469079494476,\n",
       "   0.10961417853832245,\n",
       "   0.22083410620689392,\n",
       "   0.2510247528553009,\n",
       "   0.23019792139530182,\n",
       "   0.15908633172512054,\n",
       "   0.07584737241268158,\n",
       "   0.15513871610164642,\n",
       "   -0.16032931208610535],\n",
       "  [-0.10637495666742325,\n",
       "   0.1215302050113678,\n",
       "   0.14279113709926605,\n",
       "   0.28813230991363525,\n",
       "   0.3402049243450165,\n",
       "   0.3377043902873993,\n",
       "   0.2429751753807068,\n",
       "   0.12786360085010529,\n",
       "   0.22257700562477112,\n",
       "   -0.2063126415014267],\n",
       "  [-0.16847631335258484,\n",
       "   0.12283267825841904,\n",
       "   0.18033909797668457,\n",
       "   0.34652021527290344,\n",
       "   0.40150222182273865,\n",
       "   0.43177521228790283,\n",
       "   0.33234745264053345,\n",
       "   0.2077411562204361,\n",
       "   0.29760900139808655,\n",
       "   -0.23532544076442719],\n",
       "  [-0.23343591392040253,\n",
       "   0.13489453494548798,\n",
       "   0.21264009177684784,\n",
       "   0.39830711483955383,\n",
       "   0.4422001242637634,\n",
       "   0.4987843334674835,\n",
       "   0.41326168179512024,\n",
       "   0.2947700619697571,\n",
       "   0.3689412474632263,\n",
       "   -0.26142585277557373],\n",
       "  [-0.3018285632133484,\n",
       "   0.1307247281074524,\n",
       "   0.2514607012271881,\n",
       "   0.4561883807182312,\n",
       "   0.4691040813922882,\n",
       "   0.5695794224739075,\n",
       "   0.48165783286094666,\n",
       "   0.3989560306072235,\n",
       "   0.43338218331336975,\n",
       "   -0.2604142129421234],\n",
       "  [-0.3521101176738739,\n",
       "   0.141281396150589,\n",
       "   0.271355003118515,\n",
       "   0.505827009677887,\n",
       "   0.48101359605789185,\n",
       "   0.6069961786270142,\n",
       "   0.5242564678192139,\n",
       "   0.48353955149650574,\n",
       "   0.476498544216156,\n",
       "   -0.26560845971107483],\n",
       "  [-0.39889639616012573,\n",
       "   0.14655019342899323,\n",
       "   0.2772831618785858,\n",
       "   0.5432493686676025,\n",
       "   0.5000923871994019,\n",
       "   0.6239126920700073,\n",
       "   0.5520086288452148,\n",
       "   0.5288835763931274,\n",
       "   0.5202540159225464,\n",
       "   -0.25520721077919006]],\n",
       " [[-0.007385450880974531,\n",
       "   0.025550365447998047,\n",
       "   0.011329921893775463,\n",
       "   0.03704448416829109,\n",
       "   0.039401981979608536,\n",
       "   0.027618983760476112,\n",
       "   0.017678581178188324,\n",
       "   0.025421805679798126,\n",
       "   0.01569029688835144,\n",
       "   -0.02289711870253086],\n",
       "  [-0.016434617340564728,\n",
       "   0.04202163591980934,\n",
       "   0.03387784957885742,\n",
       "   0.09133366495370865,\n",
       "   0.09891992807388306,\n",
       "   0.05646282434463501,\n",
       "   0.04472542181611061,\n",
       "   0.0426873117685318,\n",
       "   0.04018104821443558,\n",
       "   -0.05347241833806038],\n",
       "  [-0.02927660197019577,\n",
       "   0.058802951127290726,\n",
       "   0.07028387486934662,\n",
       "   0.16001532971858978,\n",
       "   0.18033723533153534,\n",
       "   0.10849107801914215,\n",
       "   0.09034308046102524,\n",
       "   0.057852793484926224,\n",
       "   0.0845632553100586,\n",
       "   -0.11631344258785248],\n",
       "  [-0.057247862219810486,\n",
       "   0.08008474111557007,\n",
       "   0.10546114295721054,\n",
       "   0.2235705405473709,\n",
       "   0.2839057445526123,\n",
       "   0.19722367823123932,\n",
       "   0.15593916177749634,\n",
       "   0.0836041271686554,\n",
       "   0.15329524874687195,\n",
       "   -0.1659594476222992],\n",
       "  [-0.10634910315275192,\n",
       "   0.09204169362783432,\n",
       "   0.14448414742946625,\n",
       "   0.2828052043914795,\n",
       "   0.3588876724243164,\n",
       "   0.30093318223953247,\n",
       "   0.23928526043891907,\n",
       "   0.14087560772895813,\n",
       "   0.22557410597801208,\n",
       "   -0.20268292725086212],\n",
       "  [-0.16713985800743103,\n",
       "   0.10960502922534943,\n",
       "   0.17833401262760162,\n",
       "   0.3384683132171631,\n",
       "   0.415674090385437,\n",
       "   0.39195284247398376,\n",
       "   0.3292734920978546,\n",
       "   0.2125479131937027,\n",
       "   0.299818217754364,\n",
       "   -0.23643545806407928],\n",
       "  [-0.23756016790866852,\n",
       "   0.10867643356323242,\n",
       "   0.2169070541858673,\n",
       "   0.4016650319099426,\n",
       "   0.45342424511909485,\n",
       "   0.4848024845123291,\n",
       "   0.4118563234806061,\n",
       "   0.31293436884880066,\n",
       "   0.3700428307056427,\n",
       "   -0.24362049996852875],\n",
       "  [-0.3006177842617035,\n",
       "   0.11638326942920685,\n",
       "   0.2345806509256363,\n",
       "   0.4584605097770691,\n",
       "   0.4678575098514557,\n",
       "   0.536447286605835,\n",
       "   0.468919575214386,\n",
       "   0.4064263701438904,\n",
       "   0.4150925278663635,\n",
       "   -0.25325480103492737],\n",
       "  [-0.3589994013309479,\n",
       "   0.12805280089378357,\n",
       "   0.2480306476354599,\n",
       "   0.5039001703262329,\n",
       "   0.47728273272514343,\n",
       "   0.5728580355644226,\n",
       "   0.5102958679199219,\n",
       "   0.48629629611968994,\n",
       "   0.4536206126213074,\n",
       "   -0.2593274414539337],\n",
       "  [-0.40764355659484863,\n",
       "   0.13997741043567657,\n",
       "   0.2571384608745575,\n",
       "   0.54050213098526,\n",
       "   0.4833797812461853,\n",
       "   0.5996587872505188,\n",
       "   0.5384678840637207,\n",
       "   0.5493736267089844,\n",
       "   0.4821021854877472,\n",
       "   -0.2614176869392395]]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, input_data_[:10, :10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dynamics_model_class as dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dmodel = dm.DynamicsModel(model_id=\"test_model\",  load_checkpoint=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 tf_venv shared",
   "language": "python",
   "name": "tf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
