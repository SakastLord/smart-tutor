{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing MCTS with Various Models\n",
    "Here we will test MCTS first with the DKT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from concept_dependency_graph import ConceptDependencyGraph\n",
    "import data_generator as dg\n",
    "from student import *\n",
    "import simple_mdp as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test2-n10000-l2-random.pickle\n"
     ]
    }
   ],
   "source": [
    "n_concepts = 2\n",
    "use_student2 = True\n",
    "student2_str = '2' if use_student2 else ''\n",
    "learn_prob = 0.5\n",
    "lp_str = '-lp{}'.format(int(learn_prob*100)) if not use_student2 else ''\n",
    "n_students = 10000\n",
    "seqlen = 2\n",
    "filter_mastery = False\n",
    "filter_str = '' if not filter_mastery else '-filtered'\n",
    "policy = 'random'\n",
    "filename = 'test{}-n{}-l{}{}-{}{}.pickle'.format(student2_str, n_students, seqlen,\n",
    "                                                    lp_str, policy, filter_str)\n",
    "#concept_tree = sm.create_custom_dependency()\n",
    "concept_tree = ConceptDependencyGraph()\n",
    "concept_tree.init_default_tree(n_concepts)\n",
    "if not use_student2:\n",
    "    test_student = Student(n=n_concepts,p_trans_satisfied=learn_prob, p_trans_not_satisfied=0.0, p_get_ex_correct_if_concepts_learned=1.0)\n",
    "else:\n",
    "    test_student = Student2(n_concepts)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing synthetic data sets...\n",
      "Generating data for 10000 students with behavior policy random and sequence length 4.\n",
      "Data generation completed. \n"
     ]
    }
   ],
   "source": [
    "print (\"Initializing synthetic data sets...\")\n",
    "dg.generate_data(concept_tree, student=test_student, n_students=n_students, filter_mastery=filter_mastery, seqlen=seqlen, policy=policy, filename=\"{}{}\".format(dg.SYN_DATA_DIR, filename))\n",
    "print (\"Data generation completed. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dynamics_model_class as dmc\n",
    "import numpy as np\n",
    "import dataset_utils\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load toy data\n",
    "data = dataset_utils.load_data(filename='{}{}'.format(dg.SYN_DATA_DIR, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average posttest: 0.62635\n",
      "Average sparse reward: 0.2527\n",
      "Percent of full posttest score: 0.2527\n",
      "Percent of all seen: 0.7478\n"
     ]
    }
   ],
   "source": [
    "print('Average posttest: {}'.format(sm.expected_reward(data)))\n",
    "print('Average sparse reward: {}'.format(sm.expected_sparse_reward(data)))\n",
    "print('Percent of full posttest score: {}'.format(sm.percent_complete(data)))\n",
    "print('Percent of all seen: {}'.format(sm.percent_all_seen(data)))\n",
    "input_data_, output_mask_, target_data_ = dataset_utils.preprocess_data_for_rnn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2, 4)\n",
      "(10000, 2, 2)\n",
      "(10000, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = (input_data_[:,:,:], output_mask_[:,:,:], target_data_[:,:,:])\n",
    "print(input_data_.shape)\n",
    "print(output_mask_.shape)\n",
    "print(target_data_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RNN dynamics model...\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test2_model2_tiny/\n",
      "Checkpoint directory path: ../checkpoints/test2_model2_tiny/\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# test_model hidden=16\n",
    "# test_model_mid hidden=10\n",
    "# test_model_small hidden=5\n",
    "# test_model_tiny hidden=3\n",
    "model_id = \"test2_model2_tiny\"\n",
    "dmodel = dmc.DynamicsModel(model_id=model_id, timesteps=seqlen, dropout=1.0, load_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtractCallback(tflearn.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.tstate = None\n",
    "    def on_epoch_end(self, training_state):\n",
    "        self.tstate = copy.copy(training_state)\n",
    "ecall = ExtractCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.07478\u001b[0m\u001b[0m | time: 1.110s\n",
      "| Adam | epoch: 002 | loss: 0.07478 -- iter: 8960/9000\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.07423\u001b[0m\u001b[0m | time: 2.122s\n",
      "| Adam | epoch: 002 | loss: 0.07423 | val_loss: 0.07013 -- iter: 9000/9000\n",
      "--\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_2.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "dmodel.train(train_data, n_epoch=2, callbacks=ecall, load_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slotnames__', '__str__', '__subclasshook__', '__weakref__', 'acc_value', 'best_accuracy', 'current_iter', 'epoch', 'global_acc', 'global_loss', 'increaseEpoch', 'increaseStep', 'loss_value', 'resetGlobal', 'step', 'step_time', 'step_time_total', 'update', 'val_acc', 'val_loss']\n",
      "0.0917459875345\n",
      "0.097650090456\n"
     ]
    }
   ],
   "source": [
    "print(dir(ecall.tstate))\n",
    "print(ecall.tstate.global_loss)\n",
    "print(ecall.tstate.val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
