{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing MCTS with Various Models\n",
    "Here we will test MCTS first with the DKT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import six\n",
    "import dynamics_model_class as dmc\n",
    "import mcts_tests as mc\n",
    "import mcts\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib.pyplot import *\n",
    "import dataset_utils\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from concept_dependency_graph import ConceptDependencyGraph\n",
    "import data_generator as dg\n",
    "from student import *\n",
    "import simple_mdp as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test2a-w5-n10000-l7-random.pickle\n"
     ]
    }
   ],
   "source": [
    "n_concepts = 5\n",
    "use_student2 = True\n",
    "transition_after = True\n",
    "student2_str = ('2' if use_student2 else '') + ('a' if use_student2 and transition_after else '')\n",
    "learn_prob = 0.5\n",
    "lp_str = '-lp{}'.format(int(learn_prob*100)) if not use_student2 else ''\n",
    "n_students = 10000\n",
    "seqlen = 7\n",
    "filter_mastery = False\n",
    "filter_str = '' if not filter_mastery else '-filtered'\n",
    "policy = 'random'\n",
    "epsilon = 0.3\n",
    "epsilon_str = '{:.2f}'.format(epsilon) if policy == 'egreedy' else ''\n",
    "filename = 'test{}-w{}-n{}-l{}{}-{}{}{}.pickle'.format(student2_str, n_concepts, n_students, seqlen,\n",
    "                                                    lp_str, policy, epsilon_str, filter_str)\n",
    "#concept_tree = sm.create_custom_dependency()\n",
    "concept_tree = ConceptDependencyGraph()\n",
    "concept_tree.init_default_tree(n_concepts)\n",
    "if not use_student2:\n",
    "    test_student = Student(n=n_concepts,p_trans_satisfied=learn_prob, p_trans_not_satisfied=0.0, p_get_ex_correct_if_concepts_learned=1.0)\n",
    "else:\n",
    "    test_student = Student2(n_concepts, transition_after=transition_after)\n",
    "six.print_(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing synthetic data sets...\n",
      "Generating data for 10000 students with behavior policy random and sequence length 7.\n",
      "Data generation completed. \n"
     ]
    }
   ],
   "source": [
    "# Generates the data\n",
    "# Only run this cell if need to generate new data, otherwise skip this\n",
    "if True:\n",
    "    print (\"Initializing synthetic data sets...\")\n",
    "    dg.generate_data(concept_tree, student=test_student, n_students=n_students, filter_mastery=filter_mastery, seqlen=seqlen, policy=policy, epsilon=epsilon, filename=\"{}{}\".format(dg.SYN_DATA_DIR, filename))\n",
    "    print (\"Data generation completed. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load toy data\n",
    "data = dataset_utils.load_data(filename='{}{}'.format(dg.SYN_DATA_DIR, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average posttest: 0.3801\n",
      "Average sparse reward: 0.0\n",
      "Percent of full posttest score: 0.0\n",
      "Percent of all seen: 0.322\n",
      "(array([0, 0, 1, 0, 0]), False, array([ 1.,  0.,  0.,  0.,  0.]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "(array([0, 0, 0, 1, 0]), False, array([ 1.,  0.,  0.,  0.,  0.]), array([1, 0, 0, 0, 0, 0, 0, 1, 0, 0]))\n",
      "(array([0, 0, 0, 0, 1]), False, array([ 1.,  0.,  0.,  0.,  0.]), array([1, 0, 0, 0, 0, 0, 0, 1, 0, 0]))\n",
      "(array([0, 1, 0, 0, 0]), False, array([ 1.,  0.,  0.,  0.,  0.]), array([1, 0, 0, 0, 0, 0, 0, 1, 0, 0]))\n",
      "(array([1, 0, 0, 0, 0]), True, array([ 1.,  0.,  0.,  0.,  0.]), array([1, 0, 0, 0, 0, 0, 1, 1, 0, 0]))\n",
      "(array([0, 0, 0, 0, 1]), False, array([ 1.,  0.,  0.,  0.,  0.]), array([1, 0, 0, 0, 0, 1, 1, 1, 0, 0]))\n",
      "(array([0, 0, 1, 0, 0]), False, array([ 1.,  0.,  1.,  0.,  0.]), array([1, 0, 0, 0, 0, 1, 1, 1, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print('Average posttest: {}'.format(sm.expected_reward(data)))\n",
    "print('Average sparse reward: {}'.format(sm.expected_sparse_reward(data)))\n",
    "print('Percent of full posttest score: {}'.format(sm.percent_complete(data)))\n",
    "print('Percent of all seen: {}'.format(sm.percent_all_seen(data)))\n",
    "for t in data[0]:\n",
    "    six.print_(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6, 10)\n",
      "(10000, 6, 5)\n",
      "(10000, 6, 5)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "[[ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "input_data_, output_mask_, target_data_ = dataset_utils.preprocess_data_for_rnn(data)\n",
    "train_data = (input_data_[:,:,:], output_mask_[:,:,:], target_data_[:,:,:])\n",
    "six.print_(input_data_.shape)\n",
    "six.print_(output_mask_.shape)\n",
    "six.print_(target_data_.shape)\n",
    "six.print_(input_data_[0,:,:])\n",
    "six.print_(output_mask_[0,:,:])\n",
    "six.print_(target_data_[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RNN dynamics model...\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test2w5_modelgrusimple_mid/\n",
      "Checkpoint directory path: ../checkpoints/test2w5_modelgrusimple_mid/\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# test_model hidden=16\n",
    "# test_model_mid hidden=10\n",
    "# test_model_small hidden=5\n",
    "# test_model_tiny hidden=3\n",
    "model_id = \"test2w5_modelgrusimple_mid\"\n",
    "dmodel = dmc.DynamicsModel(model_id=model_id, timesteps=seqlen-1, dropout=1.0, load_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtractCallback(tflearn.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.tstates = []\n",
    "    def on_epoch_end(self, training_state):\n",
    "        self.tstates.append(copy.copy(training_state))\n",
    "ecall = ExtractCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.00062\u001b[0m\u001b[0m | time: 0.563s\n",
      "| Adam | epoch: 010 | loss: 0.00062 -- iter: 09984/10000\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m0.00057\u001b[0m\u001b[0m | time: 0.566s\n",
      "| Adam | epoch: 010 | loss: 0.00057 -- iter: 10000/10000\n",
      "--\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/gru_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "dmodel.train(train_data, n_epoch=10, callbacks=ecall, load_checkpoint=False, shuffle=True, validation_set=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss Limit: 0.00324914\n",
      "Val Loss Limit: 0.00000000 Thresh: 0.00001000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd18aa7cdd0>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD/CAYAAAD2Qb01AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADylJREFUeJzt3GGIHOd9x/HvnnVSaHUnhEGhoW0EZ7dRKYoTCySf7NMU\nItrGPqyoLa4dv5CKrNZxIKkCsQnY3hf1i9ZIrUlrCZRrC3VLQMZu3AbJTdMbKZJSiZASgx3jnMBJ\nC25aHOtOkeXEvdu+eGZ1o/Xeze3eSXvR//uBxTP7n+f22Ufr3z73zMyBJEmSJEmSJEmSJEmSJEmS\nlrE+4BBwGhgHhlrqo8DZor6npba5aNN0C3CieO4YsO4q9FeStEg7gb8utjcD/1iq9QPfA9YU22eZ\nDfPPAy+RvhCacmBjsb0X2H9VeixJmldfRX0raXYOcAbYVKptACaASeBd4CQwUtQmSF8atdLx95C+\nDCB9UVzquteSpK6tqKgPAlOl/WnSl8VMUZss1S6QZv8AzwHrW37WD4v/DgMPAXd03l1J0mJVzfin\ngIGW42eK7cmW2gDwVsXPuwc4CHwceHPh3ZQkLZWqGf8p0gncI8AWZpdqAF4FbgbWAhdJyzxPzvOz\n7iet7WfM8wUxNDTUOHfuXFW/JUmzzgE3LfTgqhn/88A7pC+A/cAfA/cCD5DW9fcBL5JO4o4Bb7S0\nbxT/vQF4ClhNWgYaB+pte3/uHI1Gw0ejweOPP97zPiyHh+PgWDgW8z947xWX86qa8TeAB1uee620\n/c/Fo53XSev5kM4N3NhJxyRJV0fVjF+SdJ0x+JexLMt63YVlwXGY5VjMciy6V6s+5JprFGtWkqQF\nqNVq0EGeO+OXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAM\nfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkK\nxuCXpGAMfkkKpir4+4BDwGlgHBhqqY8CZ4v6npba5qJN003ASeAE8DRQ667LkqTFqAr+HcBKYBh4\nBNhfqvUDB4DtwDZgL7CuqH0eOAysKh1/APgCMEIK/bsX2XdJUheqgn8rcKzYPgNsKtU2ABPAJPAu\naTY/UtQmgJ1cOav/KGm2D3AU+FjXvZYkda0q+AeBqdL+dKnNICn0my4Aa4rt54D/a/lZ5S+BH5eO\nlSRdQ1XBPwUMtBw/U2xPttQGgLfm+Vkzpe0B4PwC+yhJWkIrKuqnSCdwjwBbgJdKtVeBm4G1wEXS\nMs+T8/ys/yCdCzgO/Dbw9bkOrNfrl7ezLCPLsopuSlIceZ6T53nX7auurKmRrsDZWOzvBm4FVpNO\n3t4FPEb6TWAMOFhqux74B9KJYUhfEodJJ4tfAR4AGm1es9FotHtaktROrVaDDq6UXI6XVBr8ktSB\nToPfG7gkKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KC\nMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfgl\nKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCqQr+PuAQcBoYB4Za6qPA2aK+p6LNh4CTwDeAMaC2yL5L\nkrpQFfw7gJXAMPAIsL9U6wcOANuBbcBeYF3RZlWbNnXgT4A7ivqdS/EGJEmdWVFR3wocK7bPAJtK\ntQ3ABDBZ7J8ERoDbgKNt2lwCbiTN9AeAny6m45Kk7lTN+AeBqdL+dKnNILOhD3ABWDNPmy8CTwGv\nkH4zON51ryVJXaua8U+RZudNfcBMsT3ZUhsAzs/T5hnSMs93gU+RloA+3e5F6/X65e0sy8iyrKKb\nkhRHnufked51+6oTrDtJJ3B3A1uAR5ldm+8HXgY2AxdJJ3NHSUs97dq8DtwO/BfwCeB3gPvbvGaj\n0Wh0+34kKZxarQYdXDBTNeN/nnTy9lSxvxu4F1gNHAb2AS+SZvVjwBtztIF01c+zwDvAT4AHFtpJ\nSdLSWY6XVDrjl6QOdDrj9wYuSQrG4JekYAx+SQrG4JekYAx+SQrG4JekYAx+SQrG4JekYAx+SQrG\n4JekYAx+SQrG4JekYAx+SQrG4JekYAx+SQrG4JekYAx+SQrG4JekYAx+SQrG4JekYAx+SQrG4Jek\nYAx+SQrG4JekYAx+SQrG4JekYAx+SQrG4JekYAx+SQqmKvj7gEPAaWAcGGqpjwJni/qeijbrgK8A\nx4ETwPrFdV2S1I0VFfUdwEpgGNgM7C+eA+gHDgCbgLeBU8ALwO3AqjZt/gz4O+BZIAN+HXh9qd6I\nJGlhqoJ/K3Cs2D5DCvmmDcAEMFnsnwRGgNuAo23aDAPfAb5GCvzPLKLfkqQuVS31DAJTpf3pUptB\nZkMf4AKwZo42N5CWdn4EbAd+ADzcbaclSd2rmvFPAQOl/T5gptiebKkNAOfnaDMNvElaCgL4J+CJ\nuV60Xq9f3s6yjCzLKropSXHkeU6e5123r1XUd5JO4O4GtgCPAncWtX7gZdI6/kXSydxR0lJPuzZH\nSCd3nyEt83yA9rP+RqPR6PoNSVI0tVoNqvN89vgF1J8GNhb7u4FbgdXAYeAu4DHSrH4MODhHm9eA\nXwa+BPw86TeD+7hyqajJ4JekDix18PeCwS9JHeg0+L2BS5KCMfglKRiDX5KCMfglKRiDX5KCMfgl\nKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiD\nX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCMfglKRiDX5KCqQr+PuAQcBoY\nB4Za6qPA2aK+Z4Ft7itqkqQeWFFR3wGsBIaBzcD+4jmAfuAAsAl4GzgFvADcDqyao81HgD9Yuu5L\nkjpVNePfChwrts+QQr5pAzABTALvAieBkaLN0TZtbgSeAD4L1BbbcUlSd6pm/IPAVGl/mvRlMVPU\nJku1C8CaOdqsBMaAfcA7i+uyJGkxqoJ/Chgo7TdDH1Lol2sDwPk52nwYuAk4CLwP+DXSMtG+di9a\nr9cvb2dZRpZlFd2UpDjyPCfP867bVy257CSdwN0NbAEeBe4sav3Ay6R1/IukE7ajwG3ztAH4IPDl\n4rh2Go1Go9P3IUlh1Wo16GAJvWrG/zywnXTiFlKY3wusBg6TZuwvkmb1Y8Abc7S5oo+AyS5JPbIc\nT7I645ekDnQ64/cGLkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAM\nfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkK\nxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKpir4+4BDwGlgHBhqqY8CZ4v6noo2twAniueOAesW\n2XdJUheqgn8HsBIYBh4B9pdq/cABYDuwDdhLCvMdwKo2bf4C+DTwG8BzwMNL8g4kSR1ZUVHfSpqd\nA5wBNpVqG4AJYLLYPwmMALcBR9u0+X3gv4vtfuBS172WJHWtKvgHganS/jTpt4SZojZZql0A1szT\nphn6w8BDwB1d91qS1LWqpZ4pYKDl+Jlie7KlNgCcr2hzD3AQ+DjwZnddliQtRtWM/xTpBO4RYAvw\nUqn2KnAzsBa4SFrmeRJozNHmftJ5gAx4a74Xrdfrl7ezLCPLsup3IklB5HlOnuddt68toP40sLHY\n3w3cCqwGDgN3AY+RZvVjpNl8uzbngP8Bvs/s8tBxoN7mNRuNRqPzdyJJQdVqNajO89njr15Xumbw\nS1IHOg1+b+CSpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAM\nfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGAMfkkK\nxuCXpGAMfkkKxuCXpGAMfkkKxuCXpGCqgr8POAScBsaBoZb6KHC2qO+paHMTcBI4ATwN1BbZd0lS\nF6qCfwewEhgGHgH2l2r9wAFgO7AN2AusK9qsatPmAPAFYIQU+ncvyTu4juV53usuLAuOwyzHYpZj\n0b2q4N8KHCu2zwCbSrUNwAQwCbxLms2PFG2OtmnzUdJsn6L+scV0PAI/2InjMMuxmOVYdK8q+AeB\nqdL+dKnNICn0my4Aa+ZocwNXLu38uDhWknSNVQX/FDDQcvxMsT3ZUhsAzs/RZrrUrnysJGmZ2Qn8\nTbG9BfhqqdYPvAasJZ0H+BbwC/O0eYF0LgDSyd/fm+M1J4CGDx8+fPhY8GOCJVQDDgKnisevAPcC\nDxT1u0hX9XwLeHCeNgA3Aznpap8v4VU9kiRJkiRJ16F1wH+SloSi3+RVHotbSOMwTrqkdl0P+9UL\n5bFouo+0VBhNeSzWAV8BjpM+H+t7162eKI/Fh0h58Q1gjFh58W1SNoyT3vvPVHb2A88DrwK/SjoB\nPFLUDpJuBouidSxyYGNR28uVN89d78pj0Qz+jwD/Srzgb/1c/C3wu0UtI51ni6J1LL4M/FZRe4Y4\nY/E+UvCXdZSdvf5bPU+SOvlGsR/5Jq/yWDSAe4CXilo/cKlH/eqF1s/FjcATwGdZ5jOZq6B1LIaB\nXwK+BnwS+Lce9asXWsfiEumzUSNdIv7THvXrWvsw8HPAi8DXSVdPdpSdvQz+XcD/Av9S7NeIe5PX\nLt47Fj8stoeBh4A/v/bd6oldXDkW/aRfZfeRPhOR7OK9n4v1wI9IfyrlB8DDvehYD+ziyrEA+CLw\nFPAKaQno+LXvVk9cJH0J/ibwR8Dft9SXdXYeJy1njANvkf68Q/kb+27SP2wErWPx78D7SbP+7xBr\nHbd1LKaB7xX73yTdOHigV527xtr9P/I26d4ZSOeBvtq25fWn3VicI/3pGIBPAX/Zk55deytJyz1N\nZ0l/NqfpZyY7x5ld499WPDffTV7Xs3HSuvb9pF/d1s5/+HWtORZNHySFf0TNsThC+mwAfAb40571\nqHeaefE68IvFc58grfNH8IfAXxXbHwC+S5oALDg7V1y1rnWuAXwOOEz6RnsFeLanPeqNBunf5Sng\n+8BzxfPHgXqP+rRc1EjjE9nnSDdAPkj6syf39bY7PbWHlBHvAD9h9sbS690Y6a8jNNf0dwNvYnZK\nkiRJkiRJkiRJkiRJkiRJkiRJUu/9P8WU5Q8ZB7/KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1888ed050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# binary crossentropy doesn't work\n",
    "# default batch size of 64 with 100000 trajectories is 1562.5 iters per epoch\n",
    "# which means 40 epochs is 62,500 iters\n",
    "train_loss = np.array([s.global_loss for s in ecall.tstates])\n",
    "val_loss = np.array([s.val_loss if s.val_loss is not None else 0.0 for s in ecall.tstates])\n",
    "#print(dir(ecall.tstates[0]))\n",
    "figure()\n",
    "plot(train_loss)\n",
    "plot(val_loss)\n",
    "last_val_loss = np.mean(val_loss[-10:])\n",
    "val_loss_thres = last_val_loss + 0.00001\n",
    "last_train_loss = np.mean(train_loss[-10:])\n",
    "xlim(40,50)\n",
    "six.print_('Train Loss Limit: {:.8f}'.format(last_train_loss))\n",
    "six.print_('Val Loss Limit: {:.8f} Thresh: {:.8f}'.format(last_val_loss, val_loss_thres))\n",
    "plot([0,train_loss.shape[0]], [last_val_loss,last_val_loss], color='#ff0000')\n",
    "plot([0,train_loss.shape[0]], [val_loss_thres,val_loss_thres], color='#ff0000')\n",
    "#ylim(last_val_loss - 0.00001, last_val_loss + 0.00002)\n",
    "#ylim(0,0.01)\n",
    "# training length 7\n",
    "# trying gru simple mid size with learning rate 0.01: seems like about <40 epochs is good enough\n",
    "# gru simple small size with lr 0.01: looks like 34 is enough\n",
    "# simple mid size of lr 0.01: looks like 34 is enough\n",
    "# simple small size of lr 0.01: looks like 34 is enough\n",
    "# try it out dropout=0.8\n",
    "# gru simple mid size lr 0.01: doesn't seem like 70 eps is enough, though after 40 it seems stable, val loss seems to converge much earlier\n",
    "# simple mid size lr 0.01: doesn't seem like 60 is enough, kinda stable-ish? after 30 epochs, validation loss also seems mostly stable\n",
    "# overall it seems like 40 epochs is enough for everything, small, mid, or dropout\n",
    "# training length 6\n",
    "# looks like 50 epochs is needed to stablize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]]\n",
      "(array([0, 0, 1, 0, 0]), False, array([ 1.,  0.,  1.,  0.,  0.]), array([1, 0, 0, 0, 0, 1, 0, 1, 0, 0]))\n",
      "[[[  9.89955425e-01   9.93815903e-03   3.90697550e-03   1.11842304e-02\n",
      "     1.21492986e-02]\n",
      "  [  9.96909678e-01   2.56332522e-03   1.31003105e-03   3.43835377e-03\n",
      "     3.67564568e-03]\n",
      "  [  9.96933818e-01   7.99560396e-04   2.66281962e-02   3.14442441e-03\n",
      "     3.22895916e-03]\n",
      "  [  9.97086704e-01   8.41905188e-04   2.04637181e-02   2.99884751e-03\n",
      "     3.09865386e-03]\n",
      "  [  9.97224927e-01   8.79694184e-04   1.60780381e-02   2.86530796e-03\n",
      "     2.97611044e-03]\n",
      "  [  9.97369528e-01   9.46325716e-04   1.11178551e-02   2.72916351e-03\n",
      "     2.85732327e-03]]]\n",
      "Next observation: [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Next prediction: 0.9974 0.0009 0.0111 0.0027 0.0029\n",
      "Next observation: [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Next prediction: 0.9974 0.0009 0.0111 0.0027 0.0029\n",
      "Next observation: [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Next prediction: 0.9974 0.0009 0.0111 0.0027 0.0029\n",
      "Next observation: [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "Next prediction: 0.9974 0.0009 0.0111 0.0027 0.0029\n",
      "Next observation: [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "Next prediction: 0.9974 0.0009 0.0111 0.0027 0.0029\n",
      "Next observation: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "Next prediction: 0.9974 0.0009 0.0111 0.0027 0.0029\n",
      "Next observation: [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "Next prediction: 0.9974 0.0009 0.0111 0.0027 0.0029\n",
      "Next observation: [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "Next prediction: 0.9974 0.0009 0.0111 0.0027 0.0029\n",
      "Next observation: [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "Next prediction: 0.9974 0.0009 0.0111 0.0027 0.0029\n",
      "Next observation: [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "Next prediction: 0.9974 0.0009 0.0111 0.0027 0.0029\n"
     ]
    }
   ],
   "source": [
    "# predict a bit\n",
    "dataix = 4\n",
    "test_input_data = input_data_[dataix:dataix+1,:,:]\n",
    "six.print_(test_input_data)\n",
    "six.print_(data[dataix][-1])\n",
    "prediction = dmodel.predict(test_input_data)\n",
    "six.print_(prediction)\n",
    "# add one observation of each type and see what the predictions are\n",
    "for nexta in six.moves.range(n_concepts*2):\n",
    "    obvec = np.zeros((n_concepts*2,))\n",
    "    obvec[nexta] = 1.0\n",
    "    #six.print_(test_input_data[0,:,:].shape)\n",
    "    #six.print_(obvec[np.newaxis,:].shape)\n",
    "    test_input_data2 = np.vstack((test_input_data[0,:,:],obvec[np.newaxis,:]))\n",
    "    six.print_('Next observation: {}'.format(obvec))\n",
    "    prediction = dmodel.predict(test_input_data2[np.newaxis,:,:])\n",
    "    six.print_('Next prediction: ' + ' '.join('{:.4f}'.format(x) for x in prediction[0,seqlen-2,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/gru_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# save the model to a checkpoint file\n",
    "chkpt = 'tempmodel'\n",
    "dmodel.save(chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: test2w5_modelgrusimple_mid\n",
      "horizon: 8\n",
      "rollouts: 3000\n",
      "Loading RNN dynamics model...\n",
      "Loading RNN dynamics model...\n",
      "Loading RNN dynamics model...\n",
      "Loading RNN dynamics model...\n",
      "Loading RNN dynamics model...\n",
      "Loading RNN dynamics model...\n",
      "Loading RNN dynamics model...\n",
      "Loading RNN dynamics model...\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test2w5_modelgrusimple_mid/\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test2w5_modelgrusimple_mid/\n",
      "Checkpoint directory path: ../checkpoints/test2w5_modelgrusimple_mid/\n",
      "Checkpoint directory path: ../checkpoints/test2w5_modelgrusimple_mid/\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test2w5_modelgrusimple_mid/\n",
      "Checkpoint directory path: ../checkpoints/test2w5_modelgrusimple_mid/\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test2w5_modelgrusimple_mid/\n",
      "Checkpoint directory path: ../checkpoints/test2w5_modelgrusimple_mid/\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test2w5_modelgrusimple_mid/\n",
      "Checkpoint directory path: ../checkpoints/test2w5_modelgrusimple_mid/\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test2w5_modelgrusimple_mid/\n",
      "Checkpoint directory path: ../checkpoints/test2w5_modelgrusimple_mid/\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test2w5_modelgrusimple_mid/\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/test2w5_modelgrusimple_mid/\n",
      "Checkpoint directory path: ../checkpoints/test2w5_modelgrusimple_mid/\n",
      "Checkpoint directory path: ../checkpoints/test2w5_modelgrusimple_mid/\n"
     ]
    }
   ],
   "source": [
    "# test the model on the real environment\n",
    "test_horizon = 8\n",
    "n_rollouts = 3000\n",
    "n_trajectories = 8\n",
    "r_type = mcts.SPARSE\n",
    "\n",
    "test_student = Student2(n_concepts, transition_after=transition_after)\n",
    "test_student.reset()\n",
    "test_student.knowledge[0] = 1 # initialize the first concept to be known\n",
    "sim = StudentExactSim(test_student.copy(), concept_tree)\n",
    "\n",
    "starttime = time.time()\n",
    "mc.test_dkt_chunk(n_trajectories, concept_tree, sim, model_id, chkpt, test_horizon, n_rollouts, r_type)\n",
    "#mc.test_dkt(model_id, n_concepts, transition_after, test_horizon, n_rollouts, n_trajectories, r_type, True, chkpt=chkpt)\n",
    "endtime = time.time()\n",
    "six.print_('Time Elapsed {}'.format(endtime - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
