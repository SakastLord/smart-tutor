{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import drqn\n",
    "import student as st\n",
    "\n",
    "from experience_buffer import ExperienceBuffer\n",
    "import dataset_utils as d_utils\n",
    "import utils\n",
    "import models_dict_utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data for DRQN\n",
    "We take the data from data generator and save them into traces of (s,a,r,sp) tuples.\n",
    "\n",
    "Each trajectory corresponds to a trace.\n",
    "\n",
    "If trajectory has length n, then trace will have length n-1. (since we need the next state sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = d_utils.load_data(filename=\"../synthetic_data/test-n10000-l3-random.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dqn_data = d_utils.preprocess_data_for_dqn(data, reward_model=\"dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]), array([ 1.,  0.,  0.,  0.,  0.]), 0.20000000000000001, array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])], [array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  1.]), 0.20000000000000001, array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.])]]\n"
     ]
    }
   ],
   "source": [
    "# Single Trace\n",
    "print (dqn_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "[ 1.  0.  0.  0.  0.]\n",
      "0.2\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# First tuple in a trace\n",
    "s,a,r,sp = dqn_data[0][0]\n",
    "print (s)\n",
    "print (a)\n",
    "print (r)\n",
    "print (sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  1.]\n",
      "0.2\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Last tuple\n",
    "s,a,r,sp = dqn_data[0][-1]\n",
    "print (s)\n",
    "print (a)\n",
    "print (r)\n",
    "print (sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn_data_train, dqn_data_test = train_test_split(dqn_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DRQN model and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = \"test_model_drqn\"\n",
    "\n",
    "# Directory for storing tensorboard summaries\n",
    "tensorboard_dir = '../tensorboard_logs/' + model_id + '/'\n",
    "summary_interval = 100\n",
    "checkpoint_dir = '../checkpoints/' + model_id + '/'\n",
    "checkpoint_path = checkpoint_dir + '_/'\n",
    "\n",
    "utils.check_if_path_exists_or_create(tensorboard_dir)\n",
    "utils.check_if_path_exists_or_create(checkpoint_dir)\n",
    "    \n",
    "checkpoint_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the model object\n",
    "model = drqn.DRQNModel(model_id, timesteps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize trainer object inside the model\n",
    "model.init_trainer(tensorboard_dir=tensorboard_dir, save_ckpt_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating training and validation data\n",
    "train_buffer = ExperienceBuffer()\n",
    "train_buffer.buffer = dqn_data_train\n",
    "train_buffer.buffer_sz = len(train_buffer.buffer)\n",
    "\n",
    "val_buffer = ExperienceBuffer()\n",
    "val_buffer.buffer = dqn_data_test\n",
    "val_buffer.buffer_sz = len(val_buffer.buffer)\n",
    "\n",
    "date_time_string = datetime.datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "run_id = \"{}\".format(date_time_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 38624  | total loss: \u001b[1m\u001b[32m0.08976\u001b[0m\u001b[0m | time: 2.187s\n",
      "| Optimizer | epoch: 002 | loss: 0.08976 -- iter: 7936/8000\n",
      "Training Step: 38625  | total loss: \u001b[1m\u001b[32m0.08939\u001b[0m\u001b[0m | time: 3.208s\n",
      "| Optimizer | epoch: 002 | loss: 0.08939 | val_loss: 0.08639 -- iter: 8000/8000\n",
      "--\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_2.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# train the model (uses the previously initialized trainer object)\n",
    "date_time_string = datetime.datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "run_id = \"{}\".format(date_time_string)\n",
    "model.train(train_buffer, val_buffer, n_epoch=2,\n",
    "              run_id=run_id, load_checkpoint=True,load_ckpt_path=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init evaluator of the model\n",
    "model.init_evaluator(load_ckpt_path=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create inputs (states / observations so far) to use for predictions\n",
    "from drqn import stack_batch\n",
    "train_batch = train_buffer.sample_in_order(4)\n",
    "\n",
    "# make sure that batches are over multiple timesteps, should be of shape (batch_sz, n_timesteps, ?)\n",
    "s_batch_train = stack_batch(train_batch[:, :, 0])  # current states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Use model to predict next action\n",
    "actions, q_vals = model.predict(s_batch_train, last_timestep_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30812424,  0.30810687,  0.30813327,  0.30798167,  0.30711538],\n",
       "       [ 0.43627185,  0.43623003,  0.43638146,  0.43616685,  0.43632478],\n",
       "       [ 0.43779975,  0.43772992,  0.4377695 ,  0.43766207,  0.43770874],\n",
       "       [ 0.39733657,  0.39734417,  0.39731672,  0.39723107,  0.39730671]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 tf_venv shared",
   "language": "python",
   "name": "tf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
