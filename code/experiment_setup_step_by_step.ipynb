{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lisa1010/tf_venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print sys.executable\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import concept_dependency_graph as cdg\n",
    "import dataset_utils\n",
    "import dynamics_model_class as dm\n",
    "import data_generator as dgen\n",
    "from filepaths import *\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate data\n",
    "Only needs to be done once. \n",
    "NOTE: If you already have the pickle files in the \"synthetic_data\" folder, you do NOT need to run the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_students = 10000\n",
    "seqlen = 100\n",
    "concept_tree = cdg.ConceptDependencyGraph()\n",
    "concept_tree.init_default_tree(n=N_CONCEPTS)\n",
    "print (\"Initializing synthetic data sets...\")\n",
    "for policy in ['random', 'expert', 'modulo']:\n",
    "    filename = \"{}stud_{}seq_{}.pickle\".format(n_students, seqlen, policy)\n",
    "    dgen.generate_data(concept_tree, n_students=n_students, seqlen=seqlen, policy=policy, filename=\"{}{}\".format(SYN_DATA_DIR, filename))\n",
    "print (\"Data generation completed. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data set\n",
    "Takes about 3 minutes for 10000 students with sequence length 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_name = \"10000stud_100seq_modulo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load generated data from picke files and convert it to a format so we can feed it into an RNN for training\n",
    "# NOTE: This step is not necessary if you already have the data saved in the format for RNNs. \n",
    "data = dataset_utils.load_data(filename=\"../synthetic_data/{}.pickle\".format(dataset_name))\n",
    "input_data_, output_mask_, target_data_ = dataset_utils.preprocess_data_for_rnn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save numpy matrices to files so data loading is faster, since we don't have to do the conversion again.\n",
    "dataset_utils.save_rnn_data(input_data_, output_mask_, target_data_, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the numpy matrices\n",
    "input_data_, output_mask_, target_data_ = dataset_utils.load_rnn_data(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100, 20)\n",
      "(10000, 100, 10)\n"
     ]
    }
   ],
   "source": [
    "print input_data_.shape\n",
    "print target_data_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, mask_train, mask_test, y_train, y_test = train_test_split(input_data_, output_mask_, target_data_, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = (x_train, mask_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build, train and save RNN Dynamics Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import models_dict_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each RNN model can be identified by its model_id string. \n",
    "# We will save checkpoints separately for each model. \n",
    "# Models can have different architectures, parameter dimensions etc. and are specified in models_dict.json\n",
    "model_id = \"learned_from_modulo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A model with the same model_id 'learned_from_modulo' already exists. \n",
      "No differences found. Yay! \n"
     ]
    }
   ],
   "source": [
    "# Specify input / output dimensions and hidden size\n",
    "n_timesteps = 100\n",
    "n_inputdim = 20\n",
    "n_outputdim = 10\n",
    "n_hidden = 32\n",
    "\n",
    "# If you are creating a new RNN model or just to check if it already exists:\n",
    "# Only needs to be done once for each model\n",
    "\n",
    "models_dict_utils.check_model_exists_or_create_new(model_id, n_inputdim, n_hidden, n_outputdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RNN dynamics model...\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/learned_from_modulo/\n",
      "Checkpoint directory path: ../checkpoints/learned_from_modulo/\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load model with parameters initialized randomly\n",
    "dmodel = dm.DynamicsModel(model_id=model_id, timesteps=100, load_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.01340\u001b[0m\u001b[0m | time: 12.437s\n",
      "| Adam | epoch: 002 | loss: 0.01340 -- iter: 8064/8100\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.01336\u001b[0m\u001b[0m | time: 13.544s\n",
      "| Adam | epoch: 002 | loss: 0.01336 | val_loss: 0.01319 -- iter: 8100/8100\n",
      "--\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_2.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# train model for two epochs (saves checkpoint after each epoch) \n",
    "# (checkpoint saves the weights, so we can load in pretrained models.)\n",
    "dmodel.train(train_data, n_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RNN dynamics model...\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/learned_from_modulo/\n",
      "Checkpoint directory path: ../checkpoints/learned_from_modulo/\n",
      "/Users/lisa1010/dev/smart-tutor/checkpoints/learned_from_modulo/_-254\n",
      "Checkpoint loaded.\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load model from latest checkpoint \n",
    "dmodel = dm.DynamicsModel(model_id=model_id, timesteps=100, load_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.01204\u001b[0m\u001b[0m | time: 15.510s\n",
      "| Adam | epoch: 002 | loss: 0.01204 -- iter: 8064/8100\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.01205\u001b[0m\u001b[0m | time: 16.652s\n",
      "| Adam | epoch: 002 | loss: 0.01205 | val_loss: 0.01206 -- iter: 8100/8100\n",
      "--\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_2.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# train for 2 more epochs\n",
    "dmodel.train(train_data, n_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2285  | total loss: \u001b[1m\u001b[32m0.01130\u001b[0m\u001b[0m | time: 13.150s\n",
      "| Adam | epoch: 018 | loss: 0.01130 -- iter: 8064/8100\n",
      "Training Step: 2286  | total loss: \u001b[1m\u001b[32m0.01128\u001b[0m\u001b[0m | time: 14.259s\n",
      "| Adam | epoch: 018 | loss: 0.01128 | val_loss: 0.01130 -- iter: 8100/8100\n",
      "--\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/lstm_2.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "dmodel.train(train_data, n_epoch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# important to cast preds as numpy array. \n",
    "preds = np.array(dmodel.predict(x_test[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 10)\n"
     ]
    }
   ],
   "source": [
    "print preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3591972   0.43897602  0.34933782  0.27990732  0.27030045  0.2504656\n",
      "  0.26783279  0.22879325  0.24082358  0.22392181]\n"
     ]
    }
   ],
   "source": [
    "print preds[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RNN dynamics model...\n",
      "Directory path for tensorboard summaries: ../tensorboard_logs/learned_from_modulo/\n",
      "Checkpoint directory path: ../checkpoints/learned_from_modulo/\n",
      "Checkpoint filename: /Users/lisa1010/dev/smart-tutor/checkpoints/learned_from_modulo/_-2286\n",
      "Checkpoint loaded.\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load model with different number of timesteps from checkpoint\n",
    "# Since RNN weights don't depend on # timesteps (weights are the same across time), we can load in the weights for \n",
    "# any number of timesteps. The timesteps parameter describes the # of timesteps in the input data.\n",
    "generator_model = dm.DynamicsModel(model_id=model_id, timesteps=1, load_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "preds = generator_model.predict(x_test[:1,:1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.359197199344635, 0.4389760196208954, 0.3493378162384033, 0.27990731596946716, 0.2703004479408264, 0.25046560168266296, 0.26783278584480286, 0.22879324853420258, 0.24082358181476593, 0.22392180562019348]\n"
     ]
    }
   ],
   "source": [
    "print preds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 tf_venv shared",
   "language": "python",
   "name": "tf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
